# 🤖 Agent Project Structure & Tools Documentation

*Template for building sophisticated multi-agent systems*

## 📋 Table of Contents
1. [Agent Definitions](#-agent-definitions)
2. [Agent Communication Flow](#-agent-communication-flow)
3. [Frameworks & Knowledge Base](#-frameworks--knowledge-base)
4. [Technical Architecture](#-technical-architecture)
5. [Quality Standards](#-quality-standards)
6. [Research Tools Integration](#-research-tools-integration)
7. [Performance Metrics](#-performance-metrics)
8. [Deployment & Operations](#-deployment--operations)

---

## 🎯 Agent Definitions

### **1. ICP Intelligence Agent**
**Purpose**: Deep customer psychology research and persona development
**Rules**: 
- Use 5-step reasoning chains for all insights
- Apply Eugene Schwartz awareness frameworks
- Maintain session isolation (no cross-contamination)
- Provide journal-level psychological depth

**Functions**:
- `reasoning_agent_call()` - Core psychological analysis
- `analyze_awareness_levels()` - Schwartz framework application
- `extract_voice_of_customer()` - Authentic language patterns
- `belief_system_archaeology()` - Surface/private/unconscious beliefs
- `calculate_insight_confidence()` - Evidence-based confidence scoring

**Success Outcomes**:
- Client says "How did you know that?"
- Actionable psychological insights with confidence scores >70%
- Campaign-ready customer language
- Transparent confidence methodology

**Key Metrics**:
- Research depth score (1-10)
- Average confidence score across insights
- Customer language authenticity
- Actionable insights count
- Processing time (<5 minutes)

---

### **2. Competitor Intelligence Agent**
**Purpose**: Comprehensive competitive analysis and market positioning
**Rules**:
- Use multiple data sources for validation
- Focus on messaging gaps and opportunities
- Provide strategic differentiation insights
- Maintain ethical research practices

**Functions**:
- `analyze_competitor_ads()` - Meta ads research
- `content_gap_analysis()` - SEMrush/content strategy
- `positioning_map()` - Market positioning analysis
- `opportunity_identification()` - White space detection

**Success Outcomes**:
- Clear competitive positioning strategy
- Messaging differentiation opportunities
- Market gap identification
- Competitive intelligence dashboard

**Key Metrics**:
- Competitors analyzed count
- Unique positioning angles found
- Market gaps identified
- Strategic recommendations quality

---

### **3. Interview Simulation Agent**
**Purpose**: Simulated customer interviews for authentic voice capture
**Rules**:
- Generate realistic customer personas
- Use authentic dialogue patterns
- Capture emotional language
- Validate insights through multiple personas

**Functions**:
- `generate_interview_personas()` - Create realistic customers
- `simulate_customer_dialogue()` - Generate authentic conversations
- `extract_emotional_triggers()` - Identify decision drivers
- `validate_voice_patterns()` - Confirm language authenticity

**Success Outcomes**:
- Indistinguishable from real interviews
- Authentic emotional language
- Decision trigger identification
- Voice of customer validation

**Key Metrics**:
- Interview realism score
- Emotional trigger accuracy
- Language pattern validation
- Insights correlation with real data

---

### **4. Campaign Synthesis Agent**
**Purpose**: Transform research into actionable marketing strategies
**Rules**:
- Synthesize insights from all agents
- Provide specific, testable recommendations
- Include success metrics and KPIs
- Maintain strategic coherence

**Functions**:
- `synthesize_research()` - Combine all agent outputs
- `generate_campaign_strategy()` - Create marketing plans
- `optimize_messaging()` - Refine customer language
- `create_testing_framework()` - A/B testing plans

**Success Outcomes**:
- Campaign-ready marketing strategy
- Testable messaging hierarchy
- Clear implementation roadmap
- Measurable success metrics

**Key Metrics**:
- Strategy completeness score
- Implementation clarity
- Testing framework robustness
- Expected ROI projections

---

## 🔄 Agent Communication Flow

### **Sequential Pipeline (Current)**
```
Business Context Input
         ↓
ICP Intelligence Agent
    ├── Psychology Analysis
    ├── Awareness Levels
    └── Voice of Customer
         ↓
Interview Simulation Agent
    ├── Persona Generation
    ├── Dialogue Simulation
    └── Emotional Triggers
         ↓
Campaign Synthesis Agent
    ├── Strategy Creation
    ├── Messaging Optimization
    └── Implementation Plan
         ↓
Final Research Report
```

### **Parallel Processing (Future)**
```
Business Context Input
         ↓
    ┌────────────────────┐
    ↓                    ↓
ICP Intelligence    Competitor Intelligence
    ↓                    ↓
Interview Simulation → Campaign Synthesis
         ↓
Final Integrated Report
```

### **Communication Protocols**
- **Data Format**: Structured JSON with metadata
- **Error Handling**: Graceful fallbacks and retry logic
- **State Management**: LangGraph state preservation
- **Session Isolation**: Unique session IDs
- **Quality Gates**: Validation checkpoints between agents

---

## 🧠 Frameworks & Knowledge Base

### **Psychology Frameworks**

#### **Eugene Schwartz Awareness Levels**
```
Level 1 - Unaware: Don't know they have the problem
Level 2 - Problem Aware: Know they have a problem
Level 3 - Solution Aware: Know solutions exist
Level 4 - Product Aware: Know about your product
Level 5 - Most Aware: Ready to buy
```

#### **Belief System Archaeology**
```
Surface Beliefs: What they'll tell you
Private Beliefs: What they think but won't say
Unconscious Beliefs: What drives them without awareness
```

#### **Enhanced 5-Step Reasoning Chain with Confidence**
```
1. Observation: What specific evidence do I see?
2. Pattern Recognition: What patterns does this connect to?
3. Root Cause Analysis: What deeper drivers explain this?
4. Contradiction Testing: What evidence would contradict this?
5. Confidence Assessment: Evidence quality + reliability + validation methods
```

#### **Confidence-Integrated Prompt Structure**
```
For every insight, you must provide:
1. The insight itself
2. Evidence sources supporting it
3. Reasoning chain leading to conclusion
4. Confidence score calculation showing:
   - Data quality assessment
   - Source reliability rating
   - Reasoning strength evaluation
   - Validation method coverage
5. Factors that would increase/decrease confidence
```

### **Market Research Frameworks**

#### **Market Sophistication Levels**
```
Virgin Market: First solution of its kind
Educated Market: Aware of category benefits
Experienced Market: Comparing features
Cynical Market: Skeptical of claims
```

#### **Customer Journey Mapping**
```
Awareness → Interest → Consideration → Intent → Purchase → Retention
```

### **Prompt Templates**

#### **ICP Analysis Prompt Structure**
```
SESSION ISOLATION: [Fresh analysis context]
BUSINESS CONTEXT: [Input data]
REASONING METHODOLOGY: [5-step framework]
SCHWARTZ ANALYSIS: [Awareness levels]
BELIEF ARCHAEOLOGY: [Surface/private/unconscious]
VOICE CAPTURE: [Authentic language]
OUTPUT FORMAT: [Structured response]
```

#### **Competitor Analysis Prompt Structure**
```
COMPETITIVE CONTEXT: [Market landscape]
RESEARCH METHODOLOGY: [Multi-source validation]
ANALYSIS FRAMEWORK: [Positioning, messaging, gaps]
OPPORTUNITY IDENTIFICATION: [White space detection]
STRATEGIC RECOMMENDATIONS: [Actionable insights]
```

---

## 🏗️ Technical Architecture

### **Core Technology Stack**
- **Framework**: LangChain + LangGraph
- **Models**: Claude 3.5 Sonnet (primary), GPT-4 (secondary)
- **Deployment**: Railway (auto-scaling)
- **Monitoring**: LangSmith (observability)
- **API**: FastAPI with async endpoints
- **Database**: Supabase + pgvector (future)

### **Modular Agent Architecture (Non-Coder Friendly)**

#### **Agent Auto-Discovery System**
Agents live in separate files and are automatically discovered - NO touching main.py required.

```python
# agents/__init__.py - Auto-discovery system
import os
import importlib

class AgentRegistry:
    """Automatically discovers and registers all agents"""
    
    def __init__(self):
        self.agents = {}
        self._discover_agents()
    
    def _discover_agents(self):
        """Auto-discover all agent files"""
        agents_dir = os.path.dirname(__file__)
        for file in os.listdir(agents_dir):
            if file.endswith('_agent.py') and file != '__init__.py':
                agent_name = file[:-3]  # Remove .py
                module = importlib.import_module(f'agents.{agent_name}')
                
                # Register agent if it has required interface
                if hasattr(module, 'AgentConfig') and hasattr(module, 'run_agent'):
                    self.agents[agent_name] = {
                        'config': module.AgentConfig,
                        'function': module.run_agent,
                        'module': module
                    }
    
    def get_available_agents(self):
        """Return list of all available agents"""
        return list(self.agents.keys())
    
    def run_agent(self, agent_name, input_data):
        """Run specific agent by name"""
        if agent_name in self.agents:
            return self.agents[agent_name]['function'](input_data)
        else:
            raise ValueError(f"Agent {agent_name} not found")

# Usage in main.py (never needs to change)
agent_registry = AgentRegistry()
available_agents = agent_registry.get_available_agents()  # Auto-detects all agents
```

#### **Standard Agent Template**
Every agent follows this template - copy, rename, and customize:

```python
# agents/new_agent_name_agent.py
"""
Template for creating new agents
1. Copy this file
2. Rename to your_agent_name_agent.py  
3. Customize the config and logic
4. Agent automatically appears in system
"""

# Agent Configuration (customize this)
AgentConfig = {
    "name": "Your Agent Name",
    "purpose": "What this agent does",
    "input_schema": {
        "required_fields": ["business_context"],
        "optional_fields": ["additional_context"]
    },
    "output_schema": {
        "insights": "list",
        "confidence_scores": "dict",
        "recommendations": "list"
    },
    "dependencies": ["openai", "anthropic"],  # Required packages
    "settings": {
        "model": "claude-3-5-sonnet-20241022",
        "temperature": 0.2,
        "max_tokens": 4000
    }
}

def run_agent(input_data):
    """
    Main agent function - customize this logic
    """
    try:
        # Your agent logic here
        result = process_input(input_data)
        
        # Always include confidence scoring
        from utils.confidence_scorer import ConfidenceScorer
        scorer = ConfidenceScorer()
        
        # Add confidence to all insights
        enhanced_result = add_confidence_scores(result, scorer)
        
        return {
            "status": "success",
            "agent_name": AgentConfig["name"],
            "results": enhanced_result,
            "processing_time": calculate_processing_time()
        }
        
    except Exception as e:
        return {
            "status": "error",
            "agent_name": AgentConfig["name"], 
            "error": str(e),
            "fallback_available": True
        }

def process_input(input_data):
    """Customize this function with your agent logic"""
    # Your specific agent implementation
    pass

def add_confidence_scores(result, scorer):
    """Add confidence scores to insights"""
    # Implementation for adding confidence scores
    pass
```

#### **Agent Management Dashboard (Future)**
```python
# agents/agent_manager.py
class AgentManager:
    """Manage agents without touching main.py"""
    
    def list_agents(self):
        """Show all available agents and their status"""
        return agent_registry.get_available_agents()
    
    def test_agent(self, agent_name, test_data):
        """Test individual agent with sample data"""
        return agent_registry.run_agent(agent_name, test_data)
    
    def get_agent_config(self, agent_name):
        """View agent configuration"""
        return agent_registry.agents[agent_name]['config']
    
    def validate_agent(self, agent_name):
        """Check if agent follows required interface"""
        # Validation logic
        pass
```

### **Project Structure**
```
project-root/
├── main.py                    # NEVER TOUCHED - auto-discovers agents
├── agents/
│   ├── __init__.py           # Auto-discovery system
│   ├── agent_template.py     # Copy this to create new agents
│   ├── icp_intelligence_agent.py      # ← Add agents here
│   ├── competitor_intelligence_agent.py
│   ├── interview_simulation_agent.py
│   ├── campaign_synthesis_agent.py
│   └── your_new_agent.py     # ← Just drop in new files
├── utils/
│   ├── confidence_scorer.py  # Shared utilities
│   ├── agent_validator.py
│   └── common_functions.py
├── config/
│   ├── agent_settings.json   # Global agent settings
│   └── model_configs.json
└── tests/
    ├── test_agent_template.py
    └── agent_tests/
        ├── test_icp_agent.py  # ← Auto-generated tests
        └── test_competitor_agent.py
```

### **Adding New Agents (Non-Coder Process)**

#### **Step 1: Copy Agent Template**
1. Go to `agents/agent_template.py`
2. Copy the entire file
3. Rename to `your_new_agent_name_agent.py`
4. Save in the `agents/` folder

#### **Step 2: Customize Agent Config**
Edit only the `AgentConfig` section:
```python
AgentConfig = {
    "name": "Social Media Intelligence Agent",  # ← Change this
    "purpose": "Analyze social media trends and sentiment",  # ← Change this
    "settings": {
        "model": "claude-3-5-sonnet-20241022",  # ← Model preference
        "temperature": 0.3,  # ← Creativity level (0-1)
        "max_tokens": 3000   # ← Response length
    }
}
```

#### **Step 3: Customize Agent Prompts**
Edit the prompt section:
```python
def process_input(input_data):
    prompt = """
    You are a social media intelligence expert...  # ← Your custom prompt
    
    Business Context: {business_context}
    
    Analyze social media trends and provide insights on:
    1. Platform-specific audience behavior
    2. Trending topics and hashtags  
    3. Sentiment analysis
    4. Engagement patterns
    
    Use confidence scoring for all insights.
    """
    # Rest stays the same
```

#### **Step 4: Deploy**
- Save the file
- Agent automatically appears in system
- No main.py changes needed
- Test with sample data

#### **Agent Tuning (Non-Coder Friendly)**
```python
# Tune these settings easily:
AgentConfig = {
    "settings": {
        "temperature": 0.1,     # More focused (0.0-0.2)
        "temperature": 0.5,     # Balanced (0.3-0.7) 
        "temperature": 0.9,     # More creative (0.8-1.0)
        
        "max_tokens": 2000,     # Shorter responses
        "max_tokens": 4000,     # Longer, detailed responses
        
        "model": "gpt-4o-mini",           # Faster, cheaper
        "model": "claude-3-5-sonnet-20241022"  # Higher quality
    }
}
```

---

## ⭐ Quality Standards

### **Research Quality with Confidence Transparency**
- **Psychological Depth**: Journal-level insights with evidence backing
- **Authenticity**: Voice of customer language validation + confidence scores
- **Actionability**: Specific, testable recommendations with implementation confidence
- **Coherence**: Logical flow and reasoning chains with confidence correlation
- **Completeness**: All framework elements addressed with confidence transparency
- **Evidence-Based Confidence**: No made-up scores - only evidence-backed assessments

### **Confidence Scoring System**
Each agent must provide **evidence-based confidence scores** - no made-up numbers.

#### **Confidence Calculation Methodology**
```python
class ConfidenceScorer:
    def calculate_confidence(self, insight, evidence_sources):
        # Evidence-based scoring factors
        data_quality = self.assess_data_quality(evidence_sources)
        source_reliability = self.assess_source_reliability(evidence_sources)
        reasoning_strength = self.assess_reasoning_chain(insight)
        validation_coverage = self.assess_validation_methods(insight)
        
        # Weighted confidence score (0-100)
        confidence = (
            data_quality * 0.3 +
            source_reliability * 0.25 +
            reasoning_strength * 0.25 +
            validation_coverage * 0.2
        )
        
        return {
            "confidence_score": round(confidence),
            "evidence_breakdown": {
                "data_quality": data_quality,
                "source_reliability": source_reliability,
                "reasoning_strength": reasoning_strength,
                "validation_coverage": validation_coverage
            },
            "confidence_factors": self.explain_confidence(confidence)
        }
```

#### **Evidence Sources for Confidence**
- **Data Volume**: Number of data points supporting insight
- **Source Diversity**: Multiple independent sources confirming
- **Temporal Consistency**: Insight holds across time periods
- **Cross-Validation**: Multiple methods reaching same conclusion
- **Expert Validation**: Industry knowledge confirmation
- **Statistical Significance**: Quantitative backing where applicable

#### **Confidence Score Ranges**
- **90-100%**: Multiple strong sources, clear patterns, high validation
- **70-89%**: Good evidence base, some validation, minor gaps
- **50-69%**: Moderate evidence, requires additional validation
- **30-49%**: Limited evidence, significant assumptions made
- **10-29%**: Weak evidence base, high uncertainty
- **0-9%**: Insufficient data for reliable insight

### **Quality Validation Checkpoints**
1. **Input Validation**: Business context completeness
2. **Agent Output Quality**: Framework adherence + confidence scoring
3. **Cross-Agent Consistency**: Insight alignment + confidence correlation
4. **Final Report Quality**: Client readiness assessment + confidence transparency

### **Quality Metrics**
- **Insight Depth Score**: 1-10 psychological sophistication
- **Language Authenticity**: Customer voice validation
- **Strategic Value**: Actionable recommendations count
- **Processing Efficiency**: Time to quality output
- **Client Satisfaction**: "How did you know that?" responses

---

## 🔧 Research Tools Integration

### **Customer Research Tools**

#### **Answer The Public**
- **Purpose**: Real customer questions and search patterns
- **Integration**: API calls for keyword-based question extraction
- **Usage**: Enhance voice of customer language
- **Output**: Authentic customer question patterns

#### **Google Trends**
- **Purpose**: Trending concerns and seasonal patterns
- **Integration**: pytrends library for trend analysis
- **Usage**: Market timing and emerging needs identification
- **Output**: Trend data and regional insights

### **Competitive Intelligence Tools**

#### **Meta Ads Library**
- **Purpose**: Competitor ad creative and messaging analysis
- **Integration**: Meta Ad Library API + web scraping
- **Usage**: Competitive messaging and positioning insights
- **Output**: Ad performance and messaging patterns

#### **SEMrush/Ahrefs**
- **Purpose**: Competitor content and SEO strategy
- **Integration**: API integration for content analysis
- **Usage**: Content gap identification and keyword opportunities
- **Output**: Competitive content strategy insights

#### **SimilarWeb**
- **Purpose**: Competitor traffic and audience insights
- **Integration**: API calls for traffic analysis
- **Usage**: Audience overlap and behavior patterns
- **Output**: Traffic insights and audience data

### **Tool Integration Architecture**
```python
class ResearchToolsOrchestrator:
    def __init__(self):
        self.answer_the_public = AnswerThePublicTool()
        self.google_trends = GoogleTrendsTool()
        self.meta_ads = MetaAdsResearchTool()
        self.semrush = SEMrushTool()
        
    def enhance_icp_research(self, business_context):
        # Integrate multiple tools for comprehensive analysis
        customer_questions = self.answer_the_public.get_questions()
        trending_concerns = self.google_trends.get_trends()
        return synthesized_insights
```

---

## 📊 Performance Metrics

### **System Performance**
- **Response Time**: <5 minutes for full analysis
- **Uptime**: 99.9% availability target
- **Throughput**: Concurrent request handling
- **Error Rate**: <1% failure rate

### **Research Quality**
- **Insight Accuracy**: Validation against real customer data
- **Completeness**: Framework coverage percentage
- **Uniqueness**: Novel insights vs. generic analysis
- **Actionability**: Implementation success rate

### **Business Impact**
- **Client Satisfaction**: Post-research feedback scores
- **Implementation Success**: Campaign performance metrics
- **ROI Measurement**: Revenue attribution to insights
- **Retention Rate**: Repeat client usage

---

## 🚀 Deployment & Operations

### **Deployment Pipeline**
```
GitHub → Railway Auto-Deploy → Production
    ↓
LangSmith Monitoring → Performance Tracking
    ↓
Quality Validation → Client Delivery
```

### **Monitoring & Alerting**
- **LangSmith**: Full agent tracing and performance
- **Railway**: Infrastructure monitoring
- **Custom Alerts**: Quality threshold violations
- **Error Tracking**: Detailed error logging and resolution

### **Scaling Considerations**
- **Horizontal Scaling**: Multiple Railway services
- **Caching**: Redis for repeated queries
- **Load Balancing**: Request distribution
- **Database**: Vector storage for memory systems

---

## 🔄 Continuous Improvement

### **Feedback Loops**
- **Client Feedback**: Quality and satisfaction metrics
- **Performance Monitoring**: System optimization opportunities
- **Research Validation**: Real-world result tracking
- **Model Updates**: Latest AI model integration

### **Evolution Roadmap**
1. **Foundation**: Core agent team operational
2. **Enhancement**: Research tools integration
3. **Intelligence**: Memory and learning systems
4. **Automation**: Self-improving agent capabilities
5. **Scale**: Multi-tenant and enterprise features

---

## 📚 Additional Resources

### **Documentation Standards**
- **Agent Documentation**: Purpose, rules, functions, outcomes
- **API Documentation**: Endpoint specifications and examples
- **Deployment Guides**: Step-by-step setup instructions
- **Troubleshooting**: Common issues and solutions

### **Best Practices**
- **Prompt Engineering**: Systematic prompt optimization
- **Error Handling**: Graceful failure and recovery
- **Security**: API key management and data protection
- **Testing**: Comprehensive test coverage

### **Template Usage**
This document serves as a template for building sophisticated agent systems. Copy and adapt for new projects while maintaining quality standards and architectural consistency.

---

*Last Updated: [Date]*
*Version: 1.0*
*Project: Market Research Intelligence System*
